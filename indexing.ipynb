{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "-5b4eUg5V-Oh",
    "outputId": "2917cd29-35dd-4ff3-fb00-ba2b0108cec7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob\n",
    "import matplotlib.pyplot\n",
    "files = glob.glob(\"/Users/karthi/Desktop/*.json\")\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for file in files:\n",
    "    with open(file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    segment = data[\"segments\"]\n",
    "    #print(segment)\n",
    "    \n",
    "    for segments in segment:\n",
    "        answer = segments[\"answer\"]\n",
    "        texts.append(answer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Too much talking unnessarily',\n",
       " \"it depends on the environment for nature it's day and for taking a walk it's night\",\n",
       " 'i can speak about 5 languages that includes Telugu, Tamil, English, little of french and malayalam',\n",
       " 'my fav show is topgun mavreick',\n",
       " 'iam from coimbatore',\n",
       " 'there are good people in every corner of the earth, but unfortunately the earth is round',\n",
       " 'my name is Karthi',\n",
       " ' i am ironman',\n",
       " 'iam studying B.Tech IT 3rd year',\n",
       " \"it's sunflower\",\n",
       " \"my phone's ringtone is I ain't worried\",\n",
       " 'my fav food is briyani',\n",
       " 'my fav bike is royal enfield classic',\n",
       " 'iam currently studying in coimbatore institute of technology',\n",
       " \"my pet's name is harry\",\n",
       " 'my fav flavour is bubblegum',\n",
       " \"i belive in investments as saving won't increase the value of the money, the money that has not been put to use tends to loose it's value gradually\",\n",
       " 'my fav drink in mountain dew',\n",
       " 'my fav character is Paul',\n",
       " 'i tend to learn new things that are useful to me']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(texts)):\n",
    "    ans = texts[i].split()\n",
    "    for i in range(len(ans)):\n",
    "        corpus.append(ans[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Too',\n",
       " 'much',\n",
       " 'talking',\n",
       " 'unnessarily',\n",
       " 'it',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'the',\n",
       " 'environment',\n",
       " 'for',\n",
       " 'nature',\n",
       " \"it's\",\n",
       " 'day',\n",
       " 'and',\n",
       " 'for',\n",
       " 'taking',\n",
       " 'a',\n",
       " 'walk',\n",
       " \"it's\",\n",
       " 'night',\n",
       " 'i',\n",
       " 'can',\n",
       " 'speak',\n",
       " 'about',\n",
       " '5',\n",
       " 'languages',\n",
       " 'that',\n",
       " 'includes',\n",
       " 'Telugu,',\n",
       " 'Tamil,',\n",
       " 'English,',\n",
       " 'little',\n",
       " 'of',\n",
       " 'french',\n",
       " 'and',\n",
       " 'malayalam',\n",
       " 'my',\n",
       " 'fav',\n",
       " 'show',\n",
       " 'is',\n",
       " 'topgun',\n",
       " 'mavreick',\n",
       " 'iam',\n",
       " 'from',\n",
       " 'coimbatore',\n",
       " 'there',\n",
       " 'are',\n",
       " 'good',\n",
       " 'people',\n",
       " 'in',\n",
       " 'every',\n",
       " 'corner',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earth,',\n",
       " 'but',\n",
       " 'unfortunately',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'is',\n",
       " 'round',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Karthi',\n",
       " 'i',\n",
       " 'am',\n",
       " 'ironman',\n",
       " 'iam',\n",
       " 'studying',\n",
       " 'B.Tech',\n",
       " 'IT',\n",
       " '3rd',\n",
       " 'year',\n",
       " \"it's\",\n",
       " 'sunflower',\n",
       " 'my',\n",
       " \"phone's\",\n",
       " 'ringtone',\n",
       " 'is',\n",
       " 'I',\n",
       " \"ain't\",\n",
       " 'worried',\n",
       " 'my',\n",
       " 'fav',\n",
       " 'food',\n",
       " 'is',\n",
       " 'briyani',\n",
       " 'my',\n",
       " 'fav',\n",
       " 'bike',\n",
       " 'is',\n",
       " 'royal',\n",
       " 'enfield',\n",
       " 'classic',\n",
       " 'iam',\n",
       " 'currently',\n",
       " 'studying',\n",
       " 'in',\n",
       " 'coimbatore',\n",
       " 'institute',\n",
       " 'of',\n",
       " 'technology',\n",
       " 'my',\n",
       " \"pet's\",\n",
       " 'name',\n",
       " 'is',\n",
       " 'harry',\n",
       " 'my',\n",
       " 'fav',\n",
       " 'flavour',\n",
       " 'is',\n",
       " 'bubblegum',\n",
       " 'i',\n",
       " 'belive',\n",
       " 'in',\n",
       " 'investments',\n",
       " 'as',\n",
       " 'saving',\n",
       " \"won't\",\n",
       " 'increase',\n",
       " 'the',\n",
       " 'value',\n",
       " 'of',\n",
       " 'the',\n",
       " 'money,',\n",
       " 'the',\n",
       " 'money',\n",
       " 'that',\n",
       " 'has',\n",
       " 'not',\n",
       " 'been',\n",
       " 'put',\n",
       " 'to',\n",
       " 'use',\n",
       " 'tends',\n",
       " 'to',\n",
       " 'loose',\n",
       " \"it's\",\n",
       " 'value',\n",
       " 'gradually',\n",
       " 'my',\n",
       " 'fav',\n",
       " 'drink',\n",
       " 'in',\n",
       " 'mountain',\n",
       " 'dew',\n",
       " 'my',\n",
       " 'fav',\n",
       " 'character',\n",
       " 'is',\n",
       " 'Paul',\n",
       " 'i',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'new',\n",
       " 'things',\n",
       " 'that',\n",
       " 'are',\n",
       " 'useful',\n",
       " 'to',\n",
       " 'me']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3rd', 'about', 'ain', 'am', 'and', 'are', 'as', 'been', 'belive', 'bike', 'briyani', 'bubblegum', 'but', 'can', 'character', 'classic', 'coimbatore', 'corner', 'currently', 'day', 'depends', 'dew', 'drink', 'earth', 'enfield', 'english', 'environment', 'every', 'fav', 'flavour', 'food', 'for', 'french', 'from', 'good', 'gradually', 'harry', 'has', 'iam', 'in', 'includes', 'increase', 'institute', 'investments', 'ironman', 'is', 'it', 'karthi', 'languages', 'learn', 'little', 'loose', 'malayalam', 'mavreick', 'me', 'money', 'mountain', 'much', 'my', 'name', 'nature', 'new', 'night', 'not', 'of', 'on', 'paul', 'people', 'pet', 'phone', 'put', 'ringtone', 'round', 'royal', 'saving', 'show', 'speak', 'studying', 'sunflower', 'taking', 'talking', 'tamil', 'tech', 'technology', 'telugu', 'tend', 'tends', 'that', 'the', 'there', 'things', 'to', 'too', 'topgun', 'unfortunately', 'unnessarily', 'use', 'useful', 'value', 'walk', 'won', 'worried', 'year']\n",
      "(163, 103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "hZVS9NYYWUUc",
    "outputId": "ad5810ab-c2b6-4e1c-b364-06959303cfe6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3rd</th>\n",
       "      <th>about</th>\n",
       "      <th>ain</th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>as</th>\n",
       "      <th>been</th>\n",
       "      <th>belive</th>\n",
       "      <th>bike</th>\n",
       "      <th>...</th>\n",
       "      <th>topgun</th>\n",
       "      <th>unfortunately</th>\n",
       "      <th>unnessarily</th>\n",
       "      <th>use</th>\n",
       "      <th>useful</th>\n",
       "      <th>value</th>\n",
       "      <th>walk</th>\n",
       "      <th>won</th>\n",
       "      <th>worried</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     3rd  about  ain   am  and  are   as  been  belive  bike  ...  topgun  \\\n",
       "0    0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0   0.0  ...     0.0   \n",
       "1    0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0   0.0  ...     0.0   \n",
       "2    0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0   0.0  ...     0.0   \n",
       "3    0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0   0.0  ...     0.0   \n",
       "4    0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0   0.0  ...     0.0   \n",
       "..   ...    ...  ...  ...  ...  ...  ...   ...     ...   ...  ...     ...   \n",
       "158  0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0   0.0  ...     0.0   \n",
       "159  0.0    0.0  0.0  0.0  0.0  1.0  0.0   0.0     0.0   0.0  ...     0.0   \n",
       "160  0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0   0.0  ...     0.0   \n",
       "161  0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0   0.0  ...     0.0   \n",
       "162  0.0    0.0  0.0  0.0  0.0  0.0  0.0   0.0     0.0   0.0  ...     0.0   \n",
       "\n",
       "     unfortunately  unnessarily  use  useful  value  walk  won  worried  year  \n",
       "0              0.0          0.0  0.0     0.0    0.0   0.0  0.0      0.0   0.0  \n",
       "1              0.0          0.0  0.0     0.0    0.0   0.0  0.0      0.0   0.0  \n",
       "2              0.0          0.0  0.0     0.0    0.0   0.0  0.0      0.0   0.0  \n",
       "3              0.0          1.0  0.0     0.0    0.0   0.0  0.0      0.0   0.0  \n",
       "4              0.0          0.0  0.0     0.0    0.0   0.0  0.0      0.0   0.0  \n",
       "..             ...          ...  ...     ...    ...   ...  ...      ...   ...  \n",
       "158            0.0          0.0  0.0     0.0    0.0   0.0  0.0      0.0   0.0  \n",
       "159            0.0          0.0  0.0     0.0    0.0   0.0  0.0      0.0   0.0  \n",
       "160            0.0          0.0  0.0     1.0    0.0   0.0  0.0      0.0   0.0  \n",
       "161            0.0          0.0  0.0     0.0    0.0   0.0  0.0      0.0   0.0  \n",
       "162            0.0          0.0  0.0     0.0    0.0   0.0  0.0      0.0   0.0  \n",
       "\n",
       "[163 rows x 103 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Vector Space representation\n",
    "import pandas as pd\n",
    "vector = X\n",
    "df1 = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "JKx_u_-8eVet",
    "outputId": "f548b61b-dfe3-4749-c527-94c3d512fc12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/karthi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/karthi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading basic packages\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "OxYeY6HAkNBI",
    "outputId": "fdc2dbae-fde5-4847-d367-c99646b1970b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0EDlevVigbFy"
   },
   "outputs": [],
   "source": [
    "# this function returns a list of tokenized and stemmed words of any text\n",
    "def get_tokenized_list(doc_text):\n",
    "    tokens = nltk.word_tokenize(doc_text)\n",
    "    return tokens\n",
    "\n",
    "# This function will performing stemming on tokenized words\n",
    "def word_stemmer(token_list):\n",
    "  ps = nltk.stem.PorterStemmer()\n",
    "  stemmed = []\n",
    "  for words in token_list:\n",
    "    stemmed.append(ps.stem(words))\n",
    "  return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNJX_uqujMqW"
   },
   "outputs": [],
   "source": [
    "# Function to remove stopwords from tokenized word list\n",
    "def remove_stopwords(doc_text):\n",
    "  cleaned_text = []\n",
    "  for words in doc_text:\n",
    "    if words not in stop_words:\n",
    "      cleaned_text.append(words)\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "yhvxhWsJi7u4",
    "outputId": "6f6234d9-b346-45c3-b728-9fcf80314fa4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD TOKENS:\n",
      "['much']\n",
      "\n",
      "AFTER REMOVING STOPWORDS:\n",
      "['much']\n",
      "\n",
      "AFTER PERFORMING THE WORD STEMMING::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['much']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for single document\n",
    "tokens = get_tokenized_list(corpus[1])\n",
    "print(\"WORD TOKENS:\")\n",
    "print(tokens)\n",
    "doc_text = remove_stopwords(tokens)\n",
    "print(\"\\nAFTER REMOVING STOPWORDS:\")\n",
    "print(doc_text)\n",
    "print(\"\\nAFTER PERFORMING THE WORD STEMMING::\")\n",
    "doc_text = word_stemmer(doc_text)\n",
    "doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "q987xv-5qsls",
    "outputId": "688e7883-a354-4fc0-a8a8-13e062f17681"
   },
   "outputs": [],
   "source": [
    "doc_ = ' '.join(doc_text)\n",
    "#doc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "x1XHcJr3vMx4",
    "outputId": "2d06e9a5-0c97-49d2-9519-2f1aa818aee3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['too',\n",
       " 'much',\n",
       " 'talk',\n",
       " 'unnessarili',\n",
       " '',\n",
       " 'depend',\n",
       " '',\n",
       " '',\n",
       " 'environ',\n",
       " '',\n",
       " 'natur',\n",
       " \"'s\",\n",
       " 'day',\n",
       " '',\n",
       " '',\n",
       " 'take',\n",
       " '',\n",
       " 'walk',\n",
       " \"'s\",\n",
       " 'night',\n",
       " '',\n",
       " '',\n",
       " 'speak',\n",
       " '',\n",
       " '5',\n",
       " 'languag',\n",
       " '',\n",
       " 'includ',\n",
       " 'telugu ,',\n",
       " 'tamil ,',\n",
       " 'english ,',\n",
       " 'littl',\n",
       " '',\n",
       " 'french',\n",
       " '',\n",
       " 'malayalam',\n",
       " '',\n",
       " 'fav',\n",
       " 'show',\n",
       " '',\n",
       " 'topgun',\n",
       " 'mavreick',\n",
       " 'iam',\n",
       " '',\n",
       " 'coimbator',\n",
       " '',\n",
       " '',\n",
       " 'good',\n",
       " 'peopl',\n",
       " '',\n",
       " 'everi',\n",
       " 'corner',\n",
       " '',\n",
       " '',\n",
       " 'earth ,',\n",
       " '',\n",
       " 'unfortun',\n",
       " '',\n",
       " 'earth',\n",
       " '',\n",
       " 'round',\n",
       " '',\n",
       " 'name',\n",
       " '',\n",
       " 'karthi',\n",
       " '',\n",
       " '',\n",
       " 'ironman',\n",
       " 'iam',\n",
       " 'studi',\n",
       " 'b.tech',\n",
       " 'it',\n",
       " '3rd',\n",
       " 'year',\n",
       " \"'s\",\n",
       " 'sunflow',\n",
       " '',\n",
       " \"phone 's\",\n",
       " 'rington',\n",
       " '',\n",
       " 'i',\n",
       " \"ai n't\",\n",
       " 'worri',\n",
       " '',\n",
       " 'fav',\n",
       " 'food',\n",
       " '',\n",
       " 'briyani',\n",
       " '',\n",
       " 'fav',\n",
       " 'bike',\n",
       " '',\n",
       " 'royal',\n",
       " 'enfield',\n",
       " 'classic',\n",
       " 'iam',\n",
       " 'current',\n",
       " 'studi',\n",
       " '',\n",
       " 'coimbator',\n",
       " 'institut',\n",
       " '',\n",
       " 'technolog',\n",
       " '',\n",
       " \"pet 's\",\n",
       " 'name',\n",
       " '',\n",
       " 'harri',\n",
       " '',\n",
       " 'fav',\n",
       " 'flavour',\n",
       " '',\n",
       " 'bubblegum',\n",
       " '',\n",
       " 'beliv',\n",
       " '',\n",
       " 'invest',\n",
       " '',\n",
       " 'save',\n",
       " \"wo n't\",\n",
       " 'increas',\n",
       " '',\n",
       " 'valu',\n",
       " '',\n",
       " '',\n",
       " 'money ,',\n",
       " '',\n",
       " 'money',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'put',\n",
       " '',\n",
       " 'use',\n",
       " 'tend',\n",
       " '',\n",
       " 'loos',\n",
       " \"'s\",\n",
       " 'valu',\n",
       " 'gradual',\n",
       " '',\n",
       " 'fav',\n",
       " 'drink',\n",
       " '',\n",
       " 'mountain',\n",
       " 'dew',\n",
       " '',\n",
       " 'fav',\n",
       " 'charact',\n",
       " '',\n",
       " 'paul',\n",
       " '',\n",
       " 'tend',\n",
       " '',\n",
       " 'learn',\n",
       " 'new',\n",
       " 'thing',\n",
       " '',\n",
       " '',\n",
       " 'use',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = []\n",
    "for doc in corpus:\n",
    "  tokens = get_tokenized_list(doc)\n",
    "  doc_text = remove_stopwords(tokens)\n",
    "  doc_text  = word_stemmer(doc_text)\n",
    "  doc_text = ' '.join(doc_text)\n",
    "  cleaned_corpus.append(doc_text)\n",
    "cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "XzmB6HR3x7vc",
    "outputId": "ecfe7e7b-753a-4e2b-ffd2-dbd2b7a70727",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3rd', 'ai', 'beliv', 'bike', 'briyani', 'bubblegum', 'charact', 'classic', 'coimbator', 'corner', 'current', 'day', 'depend', 'dew', 'drink', 'earth', 'enfield', 'english', 'environ', 'everi', 'fav', 'flavour', 'food', 'french', 'good', 'gradual', 'harri', 'iam', 'includ', 'increas', 'institut', 'invest', 'ironman', 'it', 'karthi', 'languag', 'learn', 'littl', 'loos', 'malayalam', 'mavreick', 'money', 'mountain', 'much', 'name', 'natur', 'new', 'night', 'paul', 'peopl', 'pet', 'phone', 'put', 'rington', 'round', 'royal', 'save', 'show', 'speak', 'studi', 'sunflow', 'take', 'talk', 'tamil', 'tech', 'technolog', 'telugu', 'tend', 'thing', 'too', 'topgun', 'unfortun', 'unnessarili', 'use', 'valu', 'walk', 'wo', 'worri', 'year']\n",
      "(163, 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizerX = TfidfVectorizer()\n",
    "vectorizerX.fit(cleaned_corpus)\n",
    "doc_vector = vectorizerX.transform(cleaned_corpus)\n",
    "print(vectorizerX.get_feature_names())\n",
    "\n",
    "print(doc_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3rd</th>\n",
       "      <th>ai</th>\n",
       "      <th>beliv</th>\n",
       "      <th>bike</th>\n",
       "      <th>briyani</th>\n",
       "      <th>bubblegum</th>\n",
       "      <th>charact</th>\n",
       "      <th>classic</th>\n",
       "      <th>coimbator</th>\n",
       "      <th>corner</th>\n",
       "      <th>...</th>\n",
       "      <th>too</th>\n",
       "      <th>topgun</th>\n",
       "      <th>unfortun</th>\n",
       "      <th>unnessarili</th>\n",
       "      <th>use</th>\n",
       "      <th>valu</th>\n",
       "      <th>walk</th>\n",
       "      <th>wo</th>\n",
       "      <th>worri</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     3rd   ai  beliv  bike  briyani  bubblegum  charact  classic  coimbator  \\\n",
       "0    0.0  0.0    0.0   0.0      0.0        0.0      0.0      0.0        0.0   \n",
       "1    0.0  0.0    0.0   0.0      0.0        0.0      0.0      0.0        0.0   \n",
       "2    0.0  0.0    0.0   0.0      0.0        0.0      0.0      0.0        0.0   \n",
       "3    0.0  0.0    0.0   0.0      0.0        0.0      0.0      0.0        0.0   \n",
       "4    0.0  0.0    0.0   0.0      0.0        0.0      0.0      0.0        0.0   \n",
       "..   ...  ...    ...   ...      ...        ...      ...      ...        ...   \n",
       "158  0.0  0.0    0.0   0.0      0.0        0.0      0.0      0.0        0.0   \n",
       "159  0.0  0.0    0.0   0.0      0.0        0.0      0.0      0.0        0.0   \n",
       "160  0.0  0.0    0.0   0.0      0.0        0.0      0.0      0.0        0.0   \n",
       "161  0.0  0.0    0.0   0.0      0.0        0.0      0.0      0.0        0.0   \n",
       "162  0.0  0.0    0.0   0.0      0.0        0.0      0.0      0.0        0.0   \n",
       "\n",
       "     corner  ...  too  topgun  unfortun  unnessarili  use  valu  walk   wo  \\\n",
       "0       0.0  ...  1.0     0.0       0.0          0.0  0.0   0.0   0.0  0.0   \n",
       "1       0.0  ...  0.0     0.0       0.0          0.0  0.0   0.0   0.0  0.0   \n",
       "2       0.0  ...  0.0     0.0       0.0          0.0  0.0   0.0   0.0  0.0   \n",
       "3       0.0  ...  0.0     0.0       0.0          1.0  0.0   0.0   0.0  0.0   \n",
       "4       0.0  ...  0.0     0.0       0.0          0.0  0.0   0.0   0.0  0.0   \n",
       "..      ...  ...  ...     ...       ...          ...  ...   ...   ...  ...   \n",
       "158     0.0  ...  0.0     0.0       0.0          0.0  0.0   0.0   0.0  0.0   \n",
       "159     0.0  ...  0.0     0.0       0.0          0.0  0.0   0.0   0.0  0.0   \n",
       "160     0.0  ...  0.0     0.0       0.0          0.0  1.0   0.0   0.0  0.0   \n",
       "161     0.0  ...  0.0     0.0       0.0          0.0  0.0   0.0   0.0  0.0   \n",
       "162     0.0  ...  0.0     0.0       0.0          0.0  0.0   0.0   0.0  0.0   \n",
       "\n",
       "     worri  year  \n",
       "0      0.0   0.0  \n",
       "1      0.0   0.0  \n",
       "2      0.0   0.0  \n",
       "3      0.0   0.0  \n",
       "4      0.0   0.0  \n",
       "..     ...   ...  \n",
       "158    0.0   0.0  \n",
       "159    0.0   0.0  \n",
       "160    0.0   0.0  \n",
       "161    0.0   0.0  \n",
       "162    0.0   0.0  \n",
       "\n",
       "[163 rows x 79 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(doc_vector.toarray(), columns=vectorizerX.get_feature_names())\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYLF1iiKibSG"
   },
   "outputs": [],
   "source": [
    "query = 'karthi'\n",
    "query = get_tokenized_list(query)\n",
    "query = remove_stopwords(query)\n",
    "q = []\n",
    "for w in word_stemmer(query):\n",
    "    q.append(w)\n",
    "q = ' '.join(q)\n",
    "q\n",
    "query_vector = vectorizerX.transform([q])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wm_V72GzJcD"
   },
   "outputs": [],
   "source": [
    "# calculate cosine similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosineSimilarities = cosine_similarity(doc_vector,query_vector).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "P_GYjPiQ1Dxp",
    "outputId": "68649962-0f81-4bed-98fa-ada7f16982b6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 64 162  59  57  56  55  54  53  52]\n",
      "['karthi']\n",
      "['']\n",
      "['']\n",
      "['']\n",
      "['unfortun']\n",
      "['']\n",
      "['earth ,']\n",
      "['']\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "related_docs_indices = cosineSimilarities.argsort()[:-10:-1]\n",
    "print(related_docs_indices)\n",
    "\n",
    "for i in related_docs_indices:\n",
    "    data = [cleaned_corpus[i]]\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VSM_the_simplest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
